{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7abf683d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "28cd3d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stacked lstm\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, n_hidden=64):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.n_hidden = n_hidden\n",
    "        \n",
    "        # layers\n",
    "        self.lstm1 = nn.LSTMCell(1, self.n_hidden)\n",
    "        self.lstm2 = nn.LSTMCell(self.n_hidden, self.n_hidden)\n",
    "        self.linear = nn.Linear(self.n_hidden, 1)\n",
    "        \n",
    "        \n",
    "    def forward(self, x, future=0):\n",
    "        outputs = []\n",
    "        h_t = torch.zeros(x.size(0), self.n_hidden, dtype=torch.float32)\n",
    "        c_t = torch.zeros(x.size(0), self.n_hidden, dtype=torch.float32)\n",
    "        h_t2 = torch.zeros(x.size(0), self.n_hidden, dtype=torch.float32)\n",
    "        c_t2 = torch.zeros(x.size(0), self.n_hidden, dtype=torch.float32)\n",
    "\n",
    "        for input_t in x.split(1, dim=1): # .split(1, dim=1)\n",
    "            h_t, c_t = self.lstm1(input_t, (h_t, c_t))\n",
    "            h_t2, c_t2 = self.lstm2(h_t, (h_t2, c_t2))\n",
    "            output = self.linear(h_t2)\n",
    "            outputs += [output]\n",
    "        for i in range(future):# if we should predict the future\n",
    "            h_t, c_t = self.lstm1(output, (h_t, c_t))\n",
    "            h_t2, c_t2 = self.lstm2(h_t, (h_t2, c_t2))\n",
    "            output = self.linear(h_t2)\n",
    "            outputs += [output]\n",
    "        outputs = torch.cat(outputs, dim=1)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ccf44b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.array([[200],[400],[600],[800],[1000],[1200],[1400],[1600],[1800],[2000]], dtype=np.float32)\n",
    "target = np.array([[80],[160],[240],[320],[400],[480.0], [560.0], [640.0], [720.0], [800.0]], dtype=np.float32)\n",
    "\n",
    "sc1 = MinMaxScaler(feature_range=(0,1))\n",
    "sc2 = MinMaxScaler(feature_range=(0,1))\n",
    "\n",
    "sc1.fit(inputs)\n",
    "sc2.fit(target)\n",
    "\n",
    "inputs = sc1.transform(inputs)\n",
    "target = sc2.transform(target)\n",
    "\n",
    "train_input = torch.from_numpy(inputs)   # [7,1], [0:7]\n",
    "train_target = torch.from_numpy(target)  # [7,1], [0:7]\n",
    "test_input = torch.from_numpy(inputs)    # [3,1], [7:]\n",
    "test_target = torch.from_numpy(target)   # [3,1], [7:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "385f1c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in training: 0.2434960901737213\n",
      "Test loss: 0.2434960901737213\n",
      "Loss in training: 8.20362583908718e-06\n",
      "Test loss: 8.20362583908718e-06\n",
      "Loss in training: 7.309389729925897e-06\n",
      "Test loss: 7.309389729925897e-06\n",
      "Loss in training: 7.296432158909738e-06\n",
      "Test loss: 7.296432158909738e-06\n",
      "Loss in training: 7.291098881978542e-06\n",
      "Test loss: 7.291098881978542e-06\n",
      "Loss in training: 7.2889524744823575e-06\n",
      "Test loss: 7.2889524744823575e-06\n",
      "Loss in training: 7.2879834078776184e-06\n",
      "Test loss: 7.2879834078776184e-06\n",
      "Loss in training: 7.287332209671149e-06\n",
      "Test loss: 7.287332209671149e-06\n",
      "Loss in training: 7.2866400842031e-06\n",
      "Test loss: 7.2866400842031e-06\n",
      "Loss in training: 7.2860566433519125e-06\n",
      "Test loss: 7.2860566433519125e-06\n",
      "x_scaled 0.37555555555555553\n",
      "pred 348.6456775665283\n",
      "y_actual 350.40000000000003\n"
     ]
    }
   ],
   "source": [
    "model = LSTM()\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.LBFGS(model.parameters(), lr=0.01)\n",
    "\n",
    "n_steps = 500\n",
    "\n",
    "# training loop\n",
    "for i in range(n_steps):    \n",
    "    def closure():\n",
    "        # zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward pass\n",
    "        out = model(train_input)\n",
    "        \n",
    "        # calculate the loss\n",
    "        loss = criterion(out, train_target)\n",
    "        \n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        return loss\n",
    "    \n",
    "    optimizer.step(closure) # update the parameters\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        future = 1\n",
    "        pred = model(test_input, future=future)\n",
    "        loss = criterion(pred[:, :-1], test_target)\n",
    "        y = pred.detach().numpy()\n",
    "        \n",
    "        if i % 50 == 0:\n",
    "            print(f\"Loss in training: {loss.item()}\")\n",
    "            print(f\"Test loss: {loss.item()}\")\n",
    "    \n",
    "    \n",
    "def predict(x):\n",
    "    x_scaled = (x - 200) / (2000 - 200)\n",
    "    y_actual = x * 0.4\n",
    "    input_t = torch.tensor([[x_scaled]])\n",
    "    pred = model(input_t)\n",
    "    pred_val = pred.detach().numpy()\n",
    "    y_scaled = pred_val.item() * (800 - 80) + 80\n",
    "    print(\"x_scaled\", x_scaled)\n",
    "    print(\"pred\", y_scaled)\n",
    "    print(\"y_actual\", y_actual)\n",
    "    \n",
    "predict(876)\n",
    "\n",
    "# x_flatten = sc.inverse_transform(test_input).flatten()\n",
    "# y_flatten = sc.inverse_transform(test_target).flatten()\n",
    "# pred_flatten = sc.inverse_transform(y[:, :-1]).flatten()\n",
    "\n",
    "# plt.figure(figsize=(12,6))\n",
    "# plt.xlabel('X')\n",
    "# plt.ylabel('Y')\n",
    "\n",
    "\n",
    "# plt.plot(x_flatten, y_flatten, 'r')\n",
    "# plt.plot(x_flatten, pred_flatten, 'b--')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4a3473d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "PATH = 'stacked_lstm.pt'\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9ee03304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and export the model to onnx\n",
    "trained_model = LSTM()\n",
    "trained_model.load_state_dict(torch.load('stacked_lstm.pt'))\n",
    "trained_model.eval()\n",
    "dummy_input = torch.tensor([[0.01]])\n",
    "torch.onnx.export(trained_model, dummy_input, 'stacked_lstm-new.onnx', verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
